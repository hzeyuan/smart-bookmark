#!/usr/bin/env python3
"""
æ™ºèƒ½ç½‘é¡µçˆ¬è™« - ä¸»ç¨‹åºå…¥å£
ç²¾ç®€é«˜æ•ˆçš„æ¶æ„ï¼Œä¸“ä¸šçº§ä»£ç ç»“æ„
"""
import asyncio
import logging
import os
from typing import Dict, Any

# åŠ è½½ç¯å¢ƒå˜é‡
from load_env import load_dotenv
load_dotenv()

from core import AutomationEngine

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


class SmartCrawler:
    """æ™ºèƒ½çˆ¬è™« - ç®€æ´çš„æ¥å£å±‚"""
    
    def __init__(self):
        # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆå”¯ä¸€ID
        from datetime import datetime
        self.task_id = f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        logger.info("ğŸš€ æ™ºèƒ½çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“‹ ä»»åŠ¡ID: {self.task_id}")
    
    async def crawl(self, instruction: str, url: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
        # è‡ªåŠ¨æ¨æ–­URL
        if not url:
            url = self._infer_url(instruction)
        
        logger.info(f"ğŸ“‹ ä»»åŠ¡: {instruction}")
        logger.info(f"ğŸŒ ç½‘ç«™: {url}")
        
        # ä½¿ç”¨AutomationEngineæ‰§è¡Œä»»åŠ¡
        async with AutomationEngine(self.task_id, headless=False) as engine:
            task_result = await engine.execute_task(instruction, url)
        
        # è½¬æ¢ä¸ºç®€æ´çš„ç»“æœæ ¼å¼
        result = {
            "success": task_result.success,
            "data": task_result.final_data,
            "error": task_result.error_message,
            "steps_taken": task_result.total_steps,
            "goal_achieved": task_result.task_state.goal_achieved if task_result.task_state else False
        }
        
        # ç®€æ´çš„ç»“æœå±•ç¤º
        if result["success"]:
            logger.info(f"âœ… æˆåŠŸæå– {len(result['data'])} æ¡æ•°æ®")
        else:
            logger.error(f"âŒ å¤±è´¥: {result['error']}")
        
        return result
    
    def _infer_url(self, instruction: str) -> str:
        """ä»æŒ‡ä»¤æ¨æ–­ç›®æ ‡ç½‘ç«™"""
        text = instruction.lower()
        
        url_map = {
            ("bilibili", "bç«™", "å“”å“©å“”å“©"): "https://www.bilibili.com",
            ("google", "è°·æ­Œ"): "https://www.google.com",
            ("github"): "https://github.com",
            ("zhihu", "çŸ¥ä¹"): "https://www.zhihu.com",
            ("baidu", "ç™¾åº¦"): "https://www.baidu.com"
        }
        
        for keywords, url in url_map.items():
            if any(keyword in text for keyword in keywords):
                return url
        
        return "https://www.google.com"


async def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸŒŸ æ™ºèƒ½ç½‘é¡µçˆ¬è™«")
    print("="*50)
    
    # æ£€æŸ¥APIé…ç½®
    if not os.getenv("OPENROUTER_API_KEY"):
        print("âŒ è¯·é…ç½® OPENROUTER_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    crawler = SmartCrawler()
    
    # é¢„å®šä¹‰ä»»åŠ¡
    tasks = [
        "åœ¨Bç«™æœç´¢Pythonæ•™ç¨‹ï¼Œè·å–å‰5ä¸ªè§†é¢‘çš„æ ‡é¢˜å’Œé“¾æ¥",
        "åœ¨Googleæœç´¢äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿ï¼Œè·å–å‰3ä¸ªç»“æœ",
        "åœ¨GitHubæœç´¢æœºå™¨å­¦ä¹ é¡¹ç›®ï¼ŒæŒ‰staræ’åºè·å–å‰5ä¸ª"
    ]
    
    print("\nğŸ¯ é¢„å®šä¹‰ä»»åŠ¡:")
    for i, task in enumerate(tasks, 1):
        print(f"{i}. {task}")
    
    print("\né€‰æ‹©:")
    print("1-3: æ‰§è¡Œé¢„å®šä¹‰ä»»åŠ¡")
    print("c: è‡ªå®šä¹‰ä»»åŠ¡")
    print("q: é€€å‡º")
    
    while True:
        choice = input("\nè¯·é€‰æ‹©: ").strip().lower()
        
        if choice == 'q':
            print("ğŸ‘‹ å†è§!")
            break
        
        elif choice in ['1', '2', '3']:
            task_idx = int(choice) - 1
            instruction = tasks[task_idx]
            
            print(f"\nğŸš€ æ‰§è¡Œä»»åŠ¡ {choice}")
            result = await crawler.crawl(instruction)
            
            if result["success"] and result["data"]:
                print(f"\nğŸ“‹ æå–çš„æ•°æ®:")
                for i, item in enumerate(result["data"][:3], 1):
                    print(f"   {i}. {item}")
            
        elif choice == 'c':
            instruction = input("è¯·è¾“å…¥ä»»åŠ¡æè¿°: ").strip()
            if instruction:
                result = await crawler.crawl(instruction)
                
                if result["success"] and result["data"]:
                    print(f"\nğŸ“‹ æå–çš„æ•°æ®:")
                    for i, item in enumerate(result["data"][:3], 1):
                        print(f"   {i}. {item}")
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    asyncio.run(main())